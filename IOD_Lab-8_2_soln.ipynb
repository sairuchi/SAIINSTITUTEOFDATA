{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYPZ2MKXMk4U"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8j6-MfKHMk4Y"
   },
   "source": [
    "# Lab 8.2: Web Scraping\n",
    "INSTRUCTIONS:\n",
    "- Read the guides and hints then create the necessary analysis and code to find an answer and conclusion for the task below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HImUFlzCMk4c"
   },
   "source": [
    "# Web Scraping in Python (using BeautifulSoup)\n",
    "\n",
    "## Scraping Rules\n",
    "1. **Always** check a website’s **Terms and Conditions** before you scrape it. Be careful to read the statements about legal use of data. Usually, the retrieved data should not be used for commercial purposes.\n",
    "2. **Do not** request data from the website too aggressively with a program (also known as spamming), as this may break the website. Make sure the program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "3. The layout of a website may change from time to time, so make sure to revisit the site and rewrite the code as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6UcvsYfMk4f"
   },
   "source": [
    "## Find a Page\n",
    "Visit the [Fandom](http://fandom.wikia.com) website, find a wikia of your interest and pick a page to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NXv6z-6Mk4i"
   },
   "source": [
    "Open a web page with the browser and inspect it.\n",
    "\n",
    "Hover the cursor on the text and follow the shaded box surrounding the main text.\n",
    "\n",
    "From the result, check the main text inside a few levels of HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYkrA5JuMk4m"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "## Import Libraries\n",
    "import regex as re\n",
    "\n",
    "from urllib.parse import unquote\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hb3_djX0Mk4s"
   },
   "source": [
    "### Define the content to retrieve (webpage's URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the url\n",
    "quote_page = 'https://bigbangtheory.fandom.com/wiki/Barry_Kripke'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKYTTGA1Mk4t"
   },
   "source": [
    "### Retrieve the page\n",
    "- Require Internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the variable 'page': bytes\n",
      "Page Retrieved. Request Status: 200, Page Size: 432374\n"
     ]
    }
   ],
   "source": [
    "# query the website and return the html to the variable ‘page’\n",
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', quote_page)\n",
    "if r.status == 200:\n",
    "    page = r.data\n",
    "    print('Type of the variable \\'page\\':', page.__class__.__name__)\n",
    "    print('Page Retrieved. Request Status: %d, Page Size: %d' % (r.status, len(page)))\n",
    "else:\n",
    "    print('Some problem occurred. Request Status: %s' % r.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7HYoOoDMk4v"
   },
   "source": [
    "### Convert the stream of bytes into a BeautifulSoup representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the variable 'soup': BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "# parse the html using beautiful soup and store in variable `soup`\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "print('Type of the variable \\'soup\\':', soup.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zb0yHBblMk4w"
   },
   "source": [
    "### Check the content\n",
    "- The HTML source\n",
    "- Includes all tags and scripts\n",
    "- Can be long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Barry Kripke | The Big Bang Theory Wiki | Fandom\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "  </script>\n",
      "  <script>\n",
      "   (window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Barry_Kripke\",\"wgTitle\":\"Barry Kripke\",\"wgCurRevisionId\":352395,\"wgRevisionId\":352395,\"wgArticleId\":2273,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Characters\",\"Caltech Faculty\",\"Scientists\",\"Physicists\",\"Experimental Physicists\",\"Theoretical Physicists\",\"Particle Physicists\",\"Recurring Characters\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\",\"Season 7\",\"Season 8\",\"Season 9\",\"The Big Bang Theory\",\"Kripke\",\"Single\",\"Sheldon\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hy2z1218Mk4x"
   },
   "source": [
    "### Check the HTML's Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title tag :<title>Barry Kripke | The Big Bang Theory Wiki | Fandom</title>:\n",
      "Title text:Barry Kripke | The Big Bang Theory Wiki | Fandom:\n"
     ]
    }
   ],
   "source": [
    "print('Title tag :%s:' % soup.title)\n",
    "print('Title text:%s:' % soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NV3IJlHQMk4z"
   },
   "source": [
    "### Find the main content\n",
    "- Check if it is possible to use only the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "article_tag = 'article'\n",
    "article = soup.find_all(article_tag)[0]\n",
    "print('Type of the variable \\'article\\':', article.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEioc2exMk40"
   },
   "source": [
    "### Get some of the text\n",
    "- Plain text without HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#show the first 500 characters after removing redundant newlines\n",
    "print(re.sub(r'\\n\\n+', '\\n', str(article)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gdf3no5jMk41"
   },
   "source": [
    "### Find the links in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#for t in article.find_all('a'):\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a filter for undesired links\n",
    "filter  = '(%s)' % '|'.join([\n",
    "    'Season_',\n",
    "    'Category:',\n",
    "    'File:',\n",
    "    'Help:',\n",
    "    'Portal:',\n",
    "    'action=',\n",
    "    'Special:',\n",
    "    'Talk:'\n",
    "])\n",
    "# remove the links that are found in the filter\n",
    "filtered_tag_list = []\n",
    "for t in wiki_tag_list:\n",
    "    if not re.search(filter, t):\n",
    "        filtered_tag_list.append(t)\n",
    "\n",
    "# filtered_tag_list = [t for t in wiki_tag_list if not re.search(filter, t)]\n",
    "print('Size of \\'filtered_tag_list\\':', len(filtered_tag_list))\n",
    "filtered_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "unique_tag_list = list(set(filtered_tag_list))\n",
    "print('Size of \\'unique_tag_list\\':', len(unique_tag_list))\n",
    "unique_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the type of tag to retrieve\n",
    "link_tag = 'a'\n",
    "\n",
    "# create a list with the links from the `<a>` tag\n",
    "tag_list = []\n",
    "for t in article.find_all(link_tag):\n",
    "    tag_list.append(t.get('href'))\n",
    "\n",
    "# List comprehension version:\n",
    "# tag_list = [t.get('href') for t in article.find_all(link_tag)]\n",
    "\n",
    "print('Size of \\'tag_list\\':', len(tag_list))\n",
    "tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RNySAm5Mk42"
   },
   "source": [
    "### Create a filter for unwanted types of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the links that start with \"The\"\n",
    "no_episodes_tag_list = []\n",
    "for tag in spaced_tag_list:\n",
    "    if not tag.startswith('The'):\n",
    "        no_episodes_tag_list.append(tag)\n",
    "\n",
    "#no_episodes_tag_list = [t for t in tag_list if not tag.startswith('The')]\n",
    "\n",
    "print('Size of \\'no_episodes_tag_list\\':', len(no_episodes_tag_list))\n",
    "no_episodes_tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab-9_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
